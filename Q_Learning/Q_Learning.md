# Q-Learning

**这篇文章将要介绍传统的qlearning算法，使用的是迭代的方法更新q表，更新q表的方法类似于向前推进，而不是使用梯度下降方法，因为这里介绍的不是Deep QLearning方法。**

## 1.1 Q(s,a)

Q(s,a)是状态动作价值函数，是在状态s 时采取动作a 之后，可以获得的奖励的期望值。Q (s, a)越大表示在agent在看到状态s 是采取动作a 比较好。

## 1.2 q_table

|      | a1       | a2       | a3       |
| ---- | -------- | -------- | -------- |
| s1   | q(s1,a1) | q(s1,a2) | q(s1,a3) |
| s2   | q(s2,a1) | q(s2,a2) | q(s2,a3) |
| s3   | q(s3,a1) | q(s3,a2) | q(s3,a3) |
| s4   | q(s4,a1) | q(s4,a2) | q(s4,a3) |

q表里面记录的都是状态动作价值函数，前面说到q表可以间接决定agent采取什么样的决策，就是因为q表记录了所有的状态和动作的组合情况，比如agent看到状态s2时，就会在状态s2所在的行选取最大的q值所对应的动作。

## 1.3 如何利用q-table决策

**假设下表是我们已经更新完成了的q表**

| q表  | a1   | a2   | a3   |
| ---- | ---- | ---- | ---- |
| s1   | - 1  | 1    | 3    |
| s2   | 2    | 0    | 1    |
| s3   | 1    | 5    | 7    |
| s4   | 5    | 6    | 3    |

Q表指导agent决策的过程：t=1时，agent观测到环境的状态s2，于是查找状态s2所在的行，发现Q(s2,a1)>Q(s2,a3)>Q(s2,a2),因此选择动a1,此时环境发生变化，agent观测到环境的状态s4，接着查找状态s4所在的行，agent发现q(s4,a2)>q(s4,a1)>q(s4,a3)，于是agent采取决策选择动作a2，一直进行下去，直到结束。

## 1.4 ε−greedy选择动作

有上面的决策过程我们可以看到，在给定一个状态s时，会选择q值最大的动作，这样会导致一个问题：因为是随机初始化的，由于选择最大值，可能会使得一些动作无法被选择到，也就是无法更新q值，这样q值就一直是随机初始化的那个值。

ε − greedy 选择动作的流程如下：agent观测到状态s时，采取动作时会以1 − ε 的概率在Q表里面选择q值最大所对应的动作，以ε的概率随机选择动作。
不再使用完全贪婪的算法，而是有一定的动作选择的完全随机性，这样就可以保证在迭代次数足够多的情况下Q表中的所有动作都会被更新到。

取0.9就是90%的可能性选择最优解，10%可能性随机选择

## 1.5 跟新q表S

![](https://img-blog.csdn.net/20180615180722209?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

`r` reward奖励值，表示当前状态转移的奖励，q_table是从当前状态转移到最后状态的奖励总和, `s'` `s`做出行动后的下一个状态，`α` 学习效率表示有多少误差要被学习，`γ` 未来奖励的衰减值
$$
\begin{aligned}
a &= E[G_t|S_t=s] \\
  &= E[R_{t+1}+λR_{t+2}+λ^2R_{t+3}+...|S_t=s] \\
  &= E[R_{t+1}+λ(R_{t+2}+λR_{t+3}+...)|S_t=s] \\
  &= E[R_{t+1}+λG_{t+1}|S_t=s] \\
  &= E[R_{t+1}+λv(S_{t+1})|S_t=s]
\end{aligned}
$$

$$
Q(s,a) = Q(s,a)+α[r + γmax_{a′}Q(s′,a′)-Q(s,a)]
$$

#### 推理过程

|      | a1   | a2   |
| ---- | ---- | ---- |
| s1   | -2   | 1    |
| s2   | -4   | 2    |

在`s1`选择a2行动，在到达`s2`后，更新q_table，然后在`s2`时并没实际行动，Q(s2,a1) < Q(s2,a2)

```
Q(s1,a2)现实 = reward + γ*maxQ(s2)
Q(s1,a2)估计 = Q(s1,a2) # 是直接根据表格估计s1,a2的值

差距 = 现实-估计
新Q(s1,a2) = 老Q(s1,a2) + α*差距 # Q值更新公式 
```


$$
Q(s1) = r2+γQ(s2) = r2 + γ[r3+γQ(s3)]=......
$$

$$
Q(s1)=r2+γr3+γ^2r4+γ^3r5+γ^4r6+......
$$



虽然使用Q(s2,a2)来更新上一个状态，但是并没有在状态`s2`进行选择采取行动，`s2`的行为决策应该等到更新完毕后再做重新另外做。

1.α 指学习率，其实也是一个权值，我们将公式1进行改写得到如下的公式
$$
Q(s,a) 
 =(1−α)Q(s,a)+α[r+γmaxa′Q(s ′,a ′)]
$$
2`γ`衰减值

首先我们随机初始化一个Q表，然后任意初始化一个状态s，也可以理解为，agent观测到的环境的状态，根据Q表使用ε−greedy算法选择状态s对应的动作a,因为agent做出了一个动作，会从环境中获得一个奖励 r ，环境发生变化，agent又观测到一个新的状态 s ′，根据Q表在状态 s ′所在行，查询Q表，求得最大值，然后更新按照公式更新Q表

可以看到，Q ( s , a ) 的更新会使用到状态s ′的Q值，从而基于TD方法进行更新Q表。但是每次更新我们都利用了下一个时刻的Q值，比如Q ( s ′ , a ′ )，假设agent完了一个episode的游戏，得到τ = { s1 , a1 , r1 , s2 , a2 , r2 , . . . , sT , aT , rT , END} ,假如我们先从后面的状态开始更新，这样前面的状态更新的时候就可以使用已经更新了的后面的状态，也就是反向更新
不知道这样是否可行，这样有个问题就是：原始的方法每一时间步都可以更新，就是在episode进行的时候更新，换成上面我说的这样的更新方法，还要将状态，动作，价值先存储起来，要等到episode结束才能更新。

可以看见Q-Learning是属于值函数近似算法中，蒙特卡洛方法和时间差分法相结合的算法

[参考链接1](https://blog.csdn.net/qq_41626059/article/details/114364666?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-114364666-blog-9361915.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-114364666-blog-9361915.pc_relevant_3mothn_strategy_recovery&utm_relevant_index=6)

# 最大似然函数

[解释](https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)

似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关母数进行猜测。

```
对同一个似然函数，其所代表的模型中，某项参数值具有多种可能，但如果存在一个参数值，使得概似函数值达到最大的话，那么这个值就是该项参数最为“合理”的参数值。
```

最大似然估计是似然函数最初也是最自然的应用。上文已经提到，似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数)或[概率质量函数](https://zh.wikipedia.org/wiki/概率质量函数)），整理之后求最大值点。实际应用中一般会取**似然函数的对数作为求最大值的函数**，这样求出的最大值点和直接求最大值点得到的结果是相同的。**似然函数的最大值点不一定唯一，也不一定存在**。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量较大。