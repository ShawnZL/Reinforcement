import gym
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import utils

class PolicyNet(torch.nn.Module):
    '''å®šä¹‰ç­–ç•¥ç½‘ç»œ'''
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        # softmax å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥å°†è¾“å…¥çš„åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼
        return F.softmax(self.fc2(x), dim = 1) # åœ¨ç¬¬äºŒä¸ªç»´åº¦è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œ
        # å°†å·¦å³ä¸¤ä¸ªæ–¹å‘é€‰æ‹©æ”¹ä¸ºå½’ä¸€åŒ–ï¼Œä¾‹å¦‚[0.4571, 0.5429] [0.4366, 0.5634]

class ValueNet(torch.nn.Module):
    '''ä»·å€¼ç½‘ç»œï¼Œè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºçŠ¶æ€çš„ä»·å€¼'''
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class ActorCritic:
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device):
        # ç­–ç•¥ç½‘ç»œ
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        # ç­–ç•¥ç½‘ç»œä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_lr)
        self.gamma = gamma
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype = torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        # æ—¶åºå·®åˆ†ç›®æ ‡
        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
        td_delta = td_target - self.critic(states) # æ—¶åºå·®åˆ†è¯¯å·®
        # gather å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œè¡¨ç¤ºå¾…é€‰å–çš„å€¼ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªç´¢å¼•å¼ é‡ï¼Œè¡¨ç¤ºå¾…é€‰å–çš„ç´¢å¼•ã€‚åœ¨è¿™é‡Œï¼Œactions æ˜¯ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†è¦é€‰æ‹©çš„åŠ¨ä½œçš„ç´¢å¼•ã€‚
        # æ ¹æ®ä»£ç ä¸­çš„ gather(1, actions)ï¼Œç¬¬äºŒä¸ªå‚æ•° actions æ˜¯ä¸€ä¸ªå¤§å°ä¸º [batch_size, 1] çš„å¼ é‡ï¼ŒåŒ…å«äº†è¦é€‰æ‹©çš„åŠ¨ä½œçš„ç´¢å¼•ã€‚
        # è€Œç¬¬ä¸€ä¸ªå‚æ•°çš„ç»´åº¦ 1 è¡¨ç¤ºè¦åœ¨ç¬¬ 1 ç»´ä¸Šè¿›è¡Œç´¢å¼•æ“ä½œï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©å¯¹åº”çš„åŠ¨ä½œæ¦‚ç‡ã€‚
        # [batch_size, num_actions] [æ‰¹é‡å¤§å°,åŠ¨ä½œçš„æ•°é‡]
<<<<<<< HEAD
        # print(actions) è¿™é‡Œæ˜¯è®°å½•æˆ‘ä»¬æ‰€é€‰æ‹©çš„åŠ¨ä½œï¼Œç„¶åä½¿ç”¨gatherè¿›è¡Œä¸€ç§ç´¢å¼•æ“ä½œ
=======
        # print(states, actions) # è¿™é‡Œæ˜¯è®°å½•æˆ‘ä»¬æ‰€é€‰æ‹©çš„åŠ¨ä½œï¼Œç„¶åä½¿ç”¨gatherè¿›è¡Œä¸€ç§ç´¢å¼•æ“ä½œ

        # ä½¿ç”¨gatherå‡½æ•°ä»æ¦‚ç‡åˆ†å¸ƒä¸­æ ¹æ®actionsç´¢å¼•æå–å¯¹åº”çš„æ¦‚ç‡å€¼ã€‚actionsæ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º(batch_size, 1)ï¼Œ
        # è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©çš„åŠ¨ä½œçš„ç´¢å¼•ã€‚gatherå‡½æ•°çš„ä½œç”¨æ˜¯æ ¹æ®ç´¢å¼•ä»æ¦‚ç‡åˆ†å¸ƒä¸­å–å‡ºå¯¹åº”çš„æ¦‚ç‡å€¼ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„å¼ é‡ã€‚
>>>>>>> f1302c7 (ğŸš€ğŸ†)
        log_probs = torch.log(self.actor(states).gather(1, actions))
        # print(log_probs)
        actor_loss = torch.mean(-log_probs * td_delta.detach())
        # å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))
        self.actor_optimizer.zero_grad() # è¿›è¡Œæ¢¯åº¦æ¸…é›¶
        self.critic_optimizer.zero_grad() # è¿›è¡Œæ¢¯åº¦æ¸…é›¶
        actor_loss.backward()  # è®¡ç®—ç­–ç•¥ç½‘ç»œçš„æ¢¯åº¦
        critic_loss.backward()  # è®¡ç®—ä»·å€¼ç½‘ç»œçš„æ¢¯åº¦
        self.actor_optimizer.step()  # æ›´æ–°ç­–ç•¥ç½‘ç»œçš„å‚æ•°
        self.critic_optimizer.step()  # æ›´æ–°ä»·å€¼ç½‘ç»œçš„å‚æ•°

actor_lr = 1e-3
critic_lr = 1e-2
num_episodes = 1000
hidden_dim = 128
gamma = 0.98
device = torch.device("cuda") if torch.cuda.is_available() else torch.device(
    "cpu")

env_name = 'CartPole-v1'
env = gym.make(env_name)
obs, info = env.reset(seed = 0)
state_dim = obs.shape[0]
action_dim = env.action_space.n
agent = ActorCritic(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
                    gamma, device)

return_list = utils.train_on_policy_agent(env, agent, num_episodes)